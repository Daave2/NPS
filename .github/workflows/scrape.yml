name: Run Playwright Scraper

# ────────────────────────────────────────────────────────────────────────────────
# PERMISSIONS  (needed for git push)
# ────────────────────────────────────────────────────────────────────────────────
permissions:
  contents: write

# ────────────────────────────────────────────────────────────────────────────────
# TRIGGERS
# ────────────────────────────────────────────────────────────────────────────────
on:
  workflow_dispatch:
  schedule:
    - cron: '0 * * * *'   # every hour

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'
  UK_TARGET_HOURS: '08 11 14 17 20'

jobs:
  check-time:
    runs-on: ubuntu-22.04
    outputs:
      run_job: ${{ steps.determine.outputs.run_job }}
    steps:
      - name: Decide whether to run scraper
        id: determine
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "run_job=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          current=$(TZ='Europe/London' date +'%H')
          if [[ " $UK_TARGET_HOURS " =~ " $current " ]]; then
            echo "run_job=true" >> $GITHUB_OUTPUT
          else
            echo "run_job=false" >> $GITHUB_OUTPUT
          fi

  scrape-and-submit:
    needs: check-time
    if: needs.check-time.outputs.run_job == 'true'
    runs-on: ubuntu-22.04
    timeout-minutes: 60

    steps:
      # checkout
      - uses: actions/checkout@v4
        with:
          persist-credentials: true
          fetch-depth: 0

      # pip cache
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # playwright browser cache
      - name: Restore Playwright browser cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-pw-browsers-v1

      # python
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      # minimal libs for chromium
      - name: Install system libs
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libnss3 libatk-bridge2.0-0 libatk1.0-0 \
            libgbm1 libgtk-3-0 libxss1 libasound2 libx11-xcb1

      # deps + chromium only
      - name: Install Python deps & Playwright Chromium
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt playwright
          python -m playwright install chromium

      # save browser cache
      - name: Save Playwright browser cache
        if: always()
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-pw-browsers-v1

      # run scraper
      - name: Run scraper
        run: python scrape.py
        env:
          GOOGLE_EMAIL:    ${{ secrets.GOOGLE_EMAIL }}
          GOOGLE_PASSWORD: ${{ secrets.GOOGLE_PASSWORD }}
          MAIN_WEBHOOK:    ${{ secrets.MAIN_WEBHOOK }}
          ALERT_WEBHOOK:   ${{ secrets.ALERT_WEBHOOK }}

      # commit updated artifacts
      - name: Commit & push updated artifacts
        if: always()
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          changed=false
          for item in comments_log.csv auth_state.json logs screens; do
            if [ -e "$item" ]; then
              git add -f "$item"
              changed=true
            fi
          done

          if [ "$changed" = true ]; then
            git diff --quiet --cached || git commit -m "ci: update logs, auth_state & comments_log"
            git push
          else
            echo "Nothing to commit."
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
